!pip install pillow
!pip install pycocotools
# Install necessary packages
!pip install torch torchvision pycocotools scikit-image

import torch
import torchvision
from torchvision.models.detection import fasterrcnn_resnet50_fpn
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from pycocotools.coco import COCO
import random
from skimage import io
import os

# Check if GPU is available and set the device accordingly
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load pre-trained Faster R-CNN model
model = fasterrcnn_resnet50_fpn(pretrained=True)
model = model.to(device)
model.eval()

# Initialize COCO dataset
DATA_PATH = "/content/coco_dataset/images/train2017"
ANN_FILE = os.path.join(DATA_PATH, '/content/coco_dataset/annotations/annotations/instances_train2017.json')
coco = COCO(ANN_FILE)
img_ids = coco.getImgIds()

# Function to get a random image and its annotations
def get_rand_img(img_ids):
    img_id = random.choice(img_ids)
    img_metadata = coco.loadImgs([img_id])[0]
    img_path = os.path.join(DATA_PATH, '/content/coco_dataset/images/train2017', img_metadata['file_name'])
    img = io.imread(img_path)
    ann_ids = coco.getAnnIds(imgIds=[img_id])
    anns = coco.loadAnns(ann_ids)
    return img, anns

# Function to display ground truth bounding boxes
def display_ground_truth(image, boxes):
    fig, ax = plt.subplots()
    ax.imshow(image)
    for box in boxes:
        rect = patches.Rectangle(
            (box['bbox'][0], box['bbox'][1]),
            box['bbox'][2], box['bbox'][3],
            linewidth=1, edgecolor='r', facecolor='none'
        )
        ax.add_patch(rect)
    ax.set_title("Ground Truth")
    plt.show()

# Function to display predicted bounding boxes
def display_predictions(image, outputs):
    fig, ax = plt.subplots()
    ax.imshow(image)
    bboxes = outputs[0]['boxes']
    scores = outputs[0]['scores']
    for bbox, score in zip(bboxes, scores):
        if score > 0.5:  # Display predictions with confidence score > 0.5
            rect = patches.Rectangle(
                (bbox[0], bbox[1]),
                bbox[2] - bbox[0], bbox[3] - bbox[1],
                linewidth=1, edgecolor='g', facecolor='none'
            )
            ax.add_patch(rect)
            ax.text(bbox[0], bbox[1] - 2, f"{score:.2f}", color='white', fontsize=12, bbox=dict(facecolor='green', alpha=0.5))
    ax.set_title("Predictions")
    plt.show()

# Get a random image and its annotations
img, anns = get_rand_img(img_ids)

# Display the image and its ground truth bounding boxes
plt.imshow(img)
display_ground_truth(img, anns)

# Preprocess the image and move it to the device
img_tensor = torchvision.transforms.functional.to_tensor(img).unsqueeze(0).to(device)

# Run inference
with torch.no_grad():
    outputs = model(img_tensor)

# Display the predicted bounding boxes
display_predictions(img, outputs)

%matplotlib inline
import torch
import torchvision
from torchvision.models.detection import ssd300_vgg16
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from pycocotools.coco import COCO
import random
from skimage import io
import os
# Load pre-trained SSD model
model = ssd300_vgg16(pretrained=True)
model = model.to(device)
model.eval()

# Initialize COCO dataset
DATA_PATH = "/content/coco_dataset/images/train2017"
ANN_FILE = os.path.join(DATA_PATH, '/content/coco_dataset/annotations/annotations/instances_train2017.json')
coco = COCO(ANN_FILE)
img_ids = coco.getImgIds()

# Function to get a random image and its annotations
def get_rand_img(img_ids):
    img_id = random.choice(img_ids)
    img_metadata = coco.loadImgs([img_id])[0]
    img_path = os.path.join(DATA_PATH, '/content/coco_dataset/images/train2017', img_metadata['file_name'])
    img = io.imread(img_path)
    ann_ids = coco.getAnnIds(imgIds=[img_id])
    anns = coco.loadAnns(ann_ids)
    return img, anns

# Function to display ground truth bounding boxes
def display_ground_truth(image, boxes):
    fig, ax = plt.subplots()
    ax.imshow(image)
    for box in boxes:
        rect = patches.Rectangle(
            (box['bbox'][0], box['bbox'][1]),
            box['bbox'][2], box['bbox'][3],
            linewidth=1, edgecolor='r', facecolor='none'
        )
        ax.add_patch(rect)
    ax.set_title("Ground Truth")
    plt.show()

# Function to display predicted bounding boxes
def display_predictions(image, outputs):
    fig, ax = plt.subplots()
    ax.imshow(image)
    bboxes = outputs[0]['boxes']
    scores = outputs[0]['scores']
    for bbox, score in zip(bboxes, scores):
        if score > 0.5:  # Display predictions with confidence score > 0.5
            rect = patches.Rectangle(
                (bbox[0], bbox[1]),
                bbox[2] - bbox[0], bbox[3] - bbox[1],
                linewidth=1, edgecolor='g', facecolor='none'
            )
            ax.add_patch(rect)
            ax.text(bbox[0], bbox[1] - 2, f"{score:.2f}", color='white', fontsize=12, bbox=dict(facecolor='green', alpha=0.5))
    ax.set_title("Predictions")
    plt.show()

# Get a random image and its annotations
img, anns = get_rand_img(img_ids)

# Display the image and its ground truth bounding boxes
plt.imshow(img)
display_ground_truth(img, anns)

# Preprocess the image and move it to the device
img_tensor = torchvision.transforms.functional.to_tensor(img).unsqueeze(0).to(device)

# Run inference
with torch.no_grad():
    outputs = model(img_tensor)

# Display the predicted bounding boxes
display_predictions(img, outputs)

# Clone the YOLOv5 repository
!git clone https://github.com/ultralytics/yolov5
%cd yolov5
# Install dependencies
!pip install -r requirements.txt

!pip install pycocotools scikit-image
import torch
from pathlib import Path
import random
from skimage import io
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from pycocotools.coco import COCO
import numpy as np

# Load YOLOv5 model
model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)
model.eval()

# Initialize COCO dataset
DATA_PATH = "/content/coco_dataset/images/train2017/"
ANN_FILE = Path("/content/coco_dataset/annotations/annotations/instances_train2017.json")
coco = COCO(ANN_FILE)
img_ids = coco.getImgIds()

%matplotlib inline
from PIL import Image
import time
import random
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np
import torch
import torchvision
from pycocotools.coco import COCO

# Load COCO dataset
DATA_PATH = "/content/coco_dataset/images/train2017/"
ANN_FILE = "/content/coco_dataset/annotations/annotations/instances_train2017.json"
coco = COCO(ANN_FILE)
img_ids = coco.getImgIds()

def get_rand_img(imgIds):
    img_id = random.choice(imgIds)
    img_metadata = coco.loadImgs([img_id])[0]
    img_path = DATA_PATH + img_metadata['file_name']
    img = Image.open(img_path).convert("RGB")
    annIds = coco.getAnnIds(imgIds=[img_id])
    anns = coco.loadAnns(annIds)
    return img, anns

def display_ground_truth(image, boxes):
    cpy_img = np.array(image.copy())  # Convert PIL image to numpy array
    fig, ax = plt.subplots()
    ax.imshow(cpy_img)
    for box in boxes:
        rect = patches.Rectangle(
            (int(box['bbox'][0]), int(box['bbox'][1])),
            int(box['bbox'][2]),
            int(box['bbox'][3]),
            linewidth=1,
            edgecolor='r',
            facecolor='none'
        )
        ax.add_patch(rect)
    ax.set_title("Ground Truth")
    plt.show()

def test_model(model, img):
    # Save the image to a temporary file
    img_path = '/content/temp.jpg'
    img.save(img_path)

    # Perform inference
    start_time = time.time()
    results = model(img_path)  # Use the model for inference
    end_time = time.time()

    # Convert the PIL image to a PyTorch tensor
    img_tensor = torchvision.transforms.ToTensor()(img)

    # Move the image tensor to the CPU
    img_array = img_tensor.permute(1, 2, 0).cpu().numpy()  # Convert to (H, W, C) format

    # Display the image with bounding boxes
    fig, ax = plt.subplots(1, 1, figsize=(12, 9))
    ax.imshow(img_array)  # Display the image

    # Iterate over the detected objects
    for result in results.xyxy[0]:
        if result[4] > 0.5:  # Show boxes with a confidence score above a threshold
            x_min, y_min, x_max, y_max = result[:4].cpu().numpy()
            rect = patches.Rectangle(
                (x_min, y_min),  # (x_min, y_min)
                x_max - x_min,  # width
                y_max - y_min,  # height
                linewidth=1,
                edgecolor='r',
                facecolor='none'
            )
            ax.add_patch(rect)

    inference_time = end_time - start_time
    return ax, inference_time

# Load a random image and its annotations
img, anns = get_rand_img(img_ids)

# Display the ground truth
display_ground_truth(img, anns)

# Load the YOLOv5 model
model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)

# Test the YOLOv5 model on the random image and display the results
ax, t = test_model(model, img)
ax.set_title(f"YOLOv5 \n Time taken: {round(t, 4)} seconds")
plt.show()


!pip install torch torchvision pycocotools matplotlib

import torch
import torchvision
import torchvision.transforms as transforms
import torchvision.models.detection as detection
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np
import time
from pycocotools.coco import COCO

# Load COCO dataset
data_transform = transforms.Compose([transforms.ToTensor()])
train_dataset = torchvision.datasets.CocoDetection(root='/content/coco_dataset/images/train2017/', annFile='/content/coco_dataset/annotations/annotations/instances_train2017.json', transform=data_transform)

# Define the model
model_retinanet = detection.retinanet_resnet50_fpn(pretrained=True)
model_retinanet.eval()

# Define a function to display ground truth bounding boxes
def display_ground_truth(image, annotations):
    fig, ax = plt.subplots(1)
    ax.imshow(image)
    for annotation in annotations:
        bbox = annotation['bbox']
        rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=1, edgecolor='r', facecolor='none')
        ax.add_patch(rect)
    plt.show()

# Define a function to display predicted bounding boxes
def display_predictions(image, predictions):
    fig, ax = plt.subplots(1)
    ax.imshow(image)
    for box in predictions['boxes']:
        box = box.detach().cpu().numpy()
        rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=1, edgecolor='b', facecolor='none')
        ax.add_patch(rect)
    plt.show()

# Define a function to get a random image and its annotations
def get_random_image_and_annotations(dataset):
    idx = np.random.randint(0, len(dataset))
    image, annotations = dataset[idx]
    return image, annotations

# Get a random image and its annotations
image, annotations = get_random_image_and_annotations(train_dataset)

# Display ground truth annotations
display_ground_truth(image.permute(1, 2, 0), annotations)

# Preprocess the image
image = image.unsqueeze(0)  # Add batch dimension
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
image = image.to(device)
model_retinanet.to(device)

# Run inference
start_time = time.time()
with torch.no_grad():
    predictions = model_retinanet(image)
end_time = time.time()

# Calculate inference time
inference_time = end_time - start_time

# Display predictions
display_predictions(image.squeeze(0).permute(1, 2, 0).cpu(), predictions[0])

# Display inference time
print(f"Inference Time: {inference_time:.4f} seconds")


!pip show yolov5
!pip install yolov5
import torch
import torchvision
import torchvision.transforms as transforms
import torchvision.models.detection as detection
import time
import numpy as np

# Function to calculate IoU
def calculate_iou(predictions, target):
    # Ensure predictions and target have at least one bounding box
    if len(predictions['boxes']) == 0 or len(target) == 0:
        return 0.0

    pred_bbox = predictions['boxes'][0].detach().cpu().numpy()
    target_bbox = target[0]['bbox']

    # Calculate the intersection area
    x_min = max(pred_bbox[0], target_bbox[0])
    y_min = max(pred_bbox[1], target_bbox[1])
    x_max = min(pred_bbox[2], target_bbox[0] + target_bbox[2])
    y_max = min(pred_bbox[3], target_bbox[1] + target_bbox[3])

    intersection_area = max(0, x_max - x_min) * max(0, y_max - y_min)

    # Calculate the union area
    pred_area = (pred_bbox[2] - pred_bbox[0]) * (pred_bbox[3] - pred_bbox[1])
    target_area = target_bbox[2] * target_bbox[3]
    union_area = pred_area + target_area - intersection_area

    # Calculate IoU
    if union_area == 0:
        return 0.0

    iou = intersection_area / union_area
    return iou

# Function to calculate mAP (dummy implementation for illustration)
def calculate_mAP(model, dataset):
    # Implement mAP calculation here (code omitted for brevity)
    return 0.0

# Load the test dataset
test_transform = transforms.Compose([transforms.ToTensor()])
test_dataset = torchvision.datasets.CocoDetection(root='/content/coco_dataset/images/train2017/', annFile='/content/coco_dataset/annotations/annotations/instances_train2017.json', transform=test_transform)

# Define the models
models = {
    "Faster R-CNN": detection.fasterrcnn_resnet50_fpn(pretrained=True),
    "SSD": detection.ssd300_vgg16(pretrained=True),
    "RetinaNet": detection.retinanet_resnet50_fpn(pretrained=True)
}

# Load YOLOv5 model
model_yolov5 = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Evaluate each model
for model_name, model in models.items():
    print(f"Evaluating {model_name}...")
    model.to(device).eval()

    # Run inference and measure inference time
    start_time = time.time()
    total_iou = 0.0
    total_images = 0
    batch_size = 4  # Adjust batch size as needed

    for i in range(0, len(test_dataset), batch_size):
        images, targets = zip(*[test_dataset[j] for j in range(i, min(i + batch_size, len(test_dataset)))])
        images = [image.to(device) for image in images]

        with torch.no_grad():
            predictions = model(images)

        for pred, target in zip(predictions, targets):
            iou = calculate_iou(pred, target)
            total_iou += iou
            total_images += 1

    end_time = time.time()
    inference_time = end_time - start_time

    # Compute mAP
    mAP = calculate_mAP(model, test_dataset)

    # Display results
    print(f"{model_name}:")
    print(f"Mean IoU: {total_iou / total_images}")
    print(f"mAP: {mAP}")
    print(f"Inference time: {inference_time} seconds")
    print("")

# Evaluate YOLOv5
print("Evaluating YOLOv5...")
model_yolov5.to(device).eval()
start_time = time.time()
total_iou = 0.0
total_images = 0

for i in range(0, len(test_dataset), batch_size):
    images, targets = zip(*[test_dataset[j] for j in range(i, min(i + batch_size, len(test_dataset)))])
    images = [image.to(device) for image in images]

    with torch.no_grad():
        predictions = model_yolov5(images)

    for pred, target in zip(predictions, targets):
        iou = calculate_iou(pred, target)
        total_iou += iou
        total_images += 1

end_time = time.time()
inference_time = end_time - start_time

# Display results for YOLOv5
print("YOLOv5:")
print(f"Mean IoU: {total_iou / total_images}")
print(f"Inference time: {inference_time} seconds")

